import argparse
import json
import logging
import os
import random
import sys
import time
from datetime import datetime, timedelta
from logging.handlers import RotatingFileHandler

import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from openai import OpenAI


def login_to_openwork(session, username, password, logger):
    """Logs in to the OpenWork website using provided credentials.

    Args:
        session (requests.Session): A requests session object.
        username (str): The OpenWork account username.
        password (str): The OpenWork account password.
        logger (logging.Logger): Logger instance for logging process details.

    Returns:
        requests.Session: The authenticated requests session object.

    Raises:
        RuntimeError: If the login page or the subsequent pages cannot be accessed.
        ValueError: If the required CSRF token is not found.
    """
    logger.info("Attempting to log in to OpenWork")
    login_page_url = "https://www.openwork.jp/login.php"
    login_check_url = "https://www.openwork.jp/login_check"
    my_top_url = "https://www.openwork.jp/my_top"

    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
            "AppleWebKit/605.1.15 (KHTML, like Gecko) Version/18.2 Safari/605.1.15"
        ),
    }

    try:
        login_page = session.get(login_page_url, headers=headers)
        login_page.raise_for_status()
    except requests.RequestException:
        logger.exception("Failed to fetch login page")
        raise RuntimeError("Could not access OpenWork login page")

    soup = BeautifulSoup(login_page.text, "html.parser")
    csrf_input = soup.find("input", {"name": "_csrf_token"})

    if not csrf_input or not csrf_input.get("value"):
        logger.error("CSRF token not found in login page")
        raise ValueError("CSRF token not found in login page")

    csrf_token = csrf_input["value"]
    payload = {
        "_username": username,
        "_password": password,
        "_remember_me": "1",
        "_csrf_token": csrf_token,
        "_target_path": "https://www.openwork.jp/",
    }

    try:
        login_response = session.post(login_check_url, data=payload, headers=headers)
        login_response.raise_for_status()

        my_top_response = session.get(my_top_url, headers=headers)
        my_top_response.raise_for_status()
    except requests.RequestException:
        logger.exception("Login request failed")
        raise RuntimeError("Failed to complete login process")

    if "ã‚ˆã†ã“ã" not in my_top_response.text:
        logger.error("Login verification failed")
        raise RuntimeError("Login failed - could not verify logged-in state")

    logger.info("Successfully logged in to OpenWork")
    return session


def get_company_info(session, m_id, logger):
    """Fetches company name and introduction from OpenWork by company ID.

    Args:
        session (requests.Session): The authenticated requests session.
        m_id (str): The company ID used in the OpenWork URL.
        logger (logging.Logger): Logger instance for logging.

    Returns:
        tuple: A tuple (company_name, company_intro), "
        "where either may be None if not found.

    Raises:
        RuntimeError: If the company page cannot be fetched.
    """
    logger.info(f"Fetching company info for ID: {m_id}")
    url = f"https://www.openwork.jp/company_answer.php?m_id={m_id}"
    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
            "AppleWebKit/605.1.15 (KHTML, like Gecko) Version/18.2 Safari/605.1.15"
        ),
    }

    try:
        response = session.get(url, headers=headers)
        response.raise_for_status()
    except requests.RequestException:
        logger.exception("Failed to fetch company info")
        raise RuntimeError(f"Could not fetch company info for ID {m_id}")

    soup = BeautifulSoup(response.text, "html.parser")
    name_tag = soup.select_one("#mainTitle > h2 > a")
    intro_tag = soup.select_one(
        "#contentsHeader_text > div > p.mt-20.w-740.madblack.break-all"
    )

    company_name = name_tag.get_text(strip=True) if name_tag else None
    company_intro = intro_tag.get_text(strip=True) if intro_tag else None

    if not company_name:
        logger.warning(f"Company name not found for ID {m_id}")
    if not company_intro:
        logger.warning(f"Company introduction not found for ID {m_id}")

    return company_name, company_intro


def scrape_reviews(session, m_id, logger, max_pages=15):
    """Scrapes reviews from OpenWork for a given company ID "
    "up to a page limit or 2-year cutoff.

    Args:
        session (requests.Session): The authenticated requests session.
        m_id (str): The company ID on OpenWork.
        logger (logging.Logger): Logger instance for logging.
        max_pages (int): The maximum number of pages to scrape.

    Returns:
        list of dict: A list of review dictionaries with keys "date" and "content".
    """
    base_url = "https://www.openwork.jp/company_answer.php"
    all_reviews = []
    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
            "AppleWebKit/605.1.15 (KHTML, like Gecko) Version/18.2 Safari/605.1.15"
        ),
        "Referer": f"https://www.openwork.jp/company_answer.php?m_id={m_id}",
    }

    cutoff_date = datetime.now() - timedelta(days=2 * 365)  # 2 years
    page = 1

    while page <= max_pages:
        logger.info(f"Scraping page {page} of maximum {max_pages}")
        params = {"m_id": m_id, "sort_key": 1, "sort_val": -1, "next_page": page}

        try:
            response = session.get(base_url, headers=headers, params=params)
            if response.status_code != 200:
                logger.warning(
                    f"Request for page {page} failed with status code "
                    "{response.status_code}"
                )
                break

            soup = BeautifulSoup(response.text, "html.parser")
            anchor = soup.select_one("#anchor01")
            if not anchor:
                logger.warning(f"No reviews found on page {page}. Stopping.")
                break

            article_list = anchor.select("article.article")
            if not article_list:
                logger.warning(f"No review data found on page {page}. Stopping.")
                break

            stop_scraping = False
            for article in article_list:
                time_tag = article.select_one("div.article_header-white > p > time")
                date_str = time_tag.get("datetime") if time_tag else None

                if date_str:
                    review_date = datetime.strptime(date_str, "%Y-%m-%d")
                    if review_date < cutoff_date:
                        logger.info(
                            f"Review dated {date_str} is older than 2 years. "
                            "Stopping further scraping."
                        )
                        stop_scraping = True
                        break

                content_dd = article.select_one(
                    "div.article_body > dl > dd.article_answer"
                )
                content_text = content_dd.get_text(strip=True) if content_dd else ""

                review_data = {"date": date_str, "content": content_text}
                all_reviews.append(review_data)

            if stop_scraping:
                break

            page += 1
            time.sleep(random.uniform(0.5, 1.0))  # Simulate human-like behavior

        except Exception:
            logger.exception(f"Error occurred on page {page}")
            break

    return all_reviews


def summarize_reviews(client, logger, company_name, company_intro, reviews):
    """Generates a structured summary from a list of reviews using the OpenAI API.

    Args:
        client (OpenAI): The OpenAI client object.
        logger (logging.Logger): Logger instance for logging.
        company_name (str): Name of the company.
        company_intro (str): Introduction or description of the company.
        reviews (list of str): The textual content of the reviews.

    Returns:
        str: A summarized text of the given reviews with a specified format.

    Raises:
        ValueError: If no reviews are provided.
        RuntimeError: If OpenAI API call fails to generate a summary.
    """
    if not reviews:
        logger.error("No reviews provided for summarization")
        raise ValueError("Cannot summarize empty reviews")

    content = "\n\n".join([f'"""\n{r}\n"""' for r in reviews])
    logger.info("Preparing to send data to OpenAI for summarization")

    prompt = (
        "éµå¾ªä»¥ä¸‹è¦æ±‚ï¼š\n"
        "1. ä¸æåŠè–ªèµ„æ°´å¹³ã€‚\n"
        "2. è¾“å‡ºä¸ä½¿ç”¨markdownæ ¼å¼ã€‚\n"
        "3. èšç„¦ä»¥ä¸‹å†…å®¹ï¼š\n"
        "   - ä¼ä¸šç»“æ„ä¸æ–‡åŒ–ç‰¹ç‚¹\n"
        "   - å·¥ä½œä¸ç”Ÿæ´»çš„å¹³è¡¡ï¼ˆWLBï¼‰\n"
        "   - å·¥ä½œæ„ä¹‰ä¸æˆé•¿æœºä¼š\n"
        "   - ä¼ä¸šçš„ä¸»è¦å¼ºé¡¹ä¸å¼±ç‚¹\n"
        "   - é€‚åˆåŠ å…¥çš„å…¸å‹äººç¾¤\n"
        "   - æœ€å¤šåˆ—å‡º 3 æ¡éœ€è¦æ³¨æ„çš„â€œæ³¨æ„ç‚¹â€\n"
        "4. å¦‚æœä½ äº†è§£å…¬å¸çš„ç›¸å…³ä¿¡æ¯ï¼Œå¯ä»¥é€‚å½“è¡¥å……ç®€ä»‹ã€‚\n"
        "5. ç¡®ä¿å„éƒ¨åˆ†å†…å®¹é€»è¾‘ä¸€è‡´ï¼Œé¿å…ç›¸äº’å†²çªã€‚\n"
        "6. æ ¼å¼å‚è€ƒå¦‚ä¸‹ï¼š\n"
        "åç§°ï¼šæè¿°\n"
        "ç®€ä»‹ï¼šæè¿°\n"
        "ã€ä¼ä¸šæ–‡åŒ–ã€‘ğŸ“\næè¿°\n"
        "ã€WLBã€‘âš–ï¸\næè¿°\n"
        "ã€æˆé•¿æœºä¼šã€‘ğŸŒ±\næè¿°\n"
        "ã€å¼ºé¡¹ä¸å¼±ç‚¹ã€‘ï¸ğŸ’ª\n- å¼ºé¡¹ï¼šæè¿°\n- å¼±ç‚¹ï¼šæè¿°\n"
        "ã€æ³¨æ„ç‚¹ã€‘âš ï¸\n- æè¿° (æœ€å¤š3ç‚¹)\n"
        "ã€é€‚åˆäººç¾¤ã€‘ğŸ‘¥\næè¿°\n"
        "ã€æ¨èæŒ‡æ•°ã€‘â­ n/5\nç»“åˆç”¨æˆ·åå¥½ï¼ˆé‡è§†WLBã€å€¾å‘å›½é™…åŒ–ä¸šåŠ¡å’Œç¯å¢ƒã€ä¸“æ³¨ITè½¯ä»¶é¢†åŸŸã€å·¥ä½œç¨³å®šä¸ä¼šè¢«è¾é€€ä¸”ä¸é¢‘ç¹è½¬ç§»ï¼‰"
        "ä¸ºè¯¥å…¬å¸ä»5æ˜Ÿä¸­ç»™å‡ºæ¨èæŒ‡æ•°å¹¶ç®€è¦è¯´æ˜ç†ç”±\n\n"
        "æ ¹æ®ä¸Šè¿°è¦æ±‚ï¼Œç«™åœ¨æ–°æ¯•ä¸šç”Ÿæ±‚èŒè€…çš„è§’åº¦ï¼Œæ•´åˆæ€»ç»“ä»¥ä¸‹å…¬å¸è¯„ä»·ï¼ˆæ¯æ¡ç”¨ä¸‰å¼•å·åŒ…å›´ï¼‰ï¼š\n\n"
        f"åç§°: {company_name}\n"
        f"ç®€ä»‹: {company_intro}\n"
        f"{content}"
    )

    try:
        response = client.chat.completions.create(
            model=args.model_name,
            messages=[
                {
                    "role": "developer",
                    "content": (
                        "ä½ æ˜¯ä¸€ä½èµ„æ·±çš„èŒä¸šé¡¾é—®ï¼Œæ“…é•¿ä½¿ç”¨ä¸­æ–‡åˆ†æå¹¶æ€»ç»“æ—¥è¯­çš„ä¼ä¸šè¯„ä»·ã€‚"
                        "ä½ çš„ç›®æ ‡æ˜¯åŸºäºæä¾›çš„è¯„ä»·å†…å®¹ï¼Œç”Ÿæˆæ¸…æ™°ã€æœ‰æ¡ç†ã€å®ç”¨çš„æ€»ç»“ï¼Œä»¥å¸®åŠ©ç”¨æˆ·åšå‡ºæ˜æ™ºçš„æ±‚èŒå†³ç­–ã€‚"
                    ),
                },
                {
                    "role": "user",
                    "content": prompt,
                },
            ],
            temperature=1.0,
            top_p=1.0,
        )
    except Exception:
        logger.exception("OpenAI API error")
        raise RuntimeError("Failed to generate summary using OpenAI API")

    summary = response.choices[0].message.content
    logger.info("Successfully generated summary")
    return summary


def setup_logging():
    """Sets up the logging configuration for the script.

    Returns:
        logging.Logger: Configured logger instance.
    """
    logger = logging.getLogger("openwork_review_summarizer")
    logger.setLevel(logging.DEBUG)
    logger.handlers.clear()

    formatter = logging.Formatter(
        fmt="%(asctime)s - %(levelname)s - %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
    )

    file_handler = RotatingFileHandler(
        filename="openwork_review_summarizer.log",
        maxBytes=5 * 1024 * 1024,  # 5 MB
        backupCount=3,
        encoding="utf-8",
    )
    file_handler.setLevel(logging.DEBUG)
    file_handler.setFormatter(formatter)
    logger.addHandler(file_handler)

    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)

    return logger


def safe_filename(name):
    """Converts a given string into a filename-safe format.

    Args:
        name (str): The original name (e.g., company name).

    Returns:
        str: A sanitized filename string containing only alphanumeric, "
        "hyphens, or underscores.
    """
    return "".join(c if c.isalnum() or c in ("-", "_") else "_" for c in name)


def parse_args():
    """Parses command-line arguments.

    Returns:
        argparse.Namespace: The parsed command-line arguments.
    """
    parser = argparse.ArgumentParser(
        description="OpenWork Review Summarizer using OpenAI models. "
        "Requires OpenAI API key and OpenWork user account "
        "with access to full reviews.",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )

    parser.add_argument(
        "--company-id",
        help="Company ID to scrape. If omitted, you'll be prompted for it.",
    )
    parser.add_argument(
        "--username",
        help="Your OpenWork username. If omitted, will try environment or .env file.",
    )
    parser.add_argument(
        "--password",
        help="Your OpenWork password. If omitted, will try environment or .env file.",
    )
    parser.add_argument(
        "--model-name",
        default="gpt-4o",
        help="OpenAI model to use for summarization. 4o is recommended for this task.",
    )

    return parser.parse_args()


if __name__ == "__main__":
    logger = setup_logging()

    args = parse_args()
    load_dotenv()

    os.makedirs("reviews", exist_ok=True)
    os.makedirs("summaries", exist_ok=True)

    try:
        username = args.username or os.getenv("OPENWORK_USERNAME")
        password = args.password or os.getenv("OPENWORK_PASSWORD")
        api_key = os.getenv("OPENAI_API_KEY")
        project_id = os.getenv("OPENAI_PROJECT_ID")  # optional usage

        if not all([api_key, username, password]):
            logger.error(
                "Missing required credentials (API key, username, or password)."
            )
            raise ValueError("Required credentials are not set (check CLI or env)")

        client = OpenAI(api_key=api_key)

        company_id = args.company_id
        if not company_id:
            company_id = input("Enter the company ID: ").strip()
            if not company_id:
                logger.error("No company ID provided")
                raise ValueError("Company ID is required")

        with requests.Session() as session:
            session = login_to_openwork(session, username, password, logger)
            company_name, company_intro = get_company_info(session, company_id, logger)

            if not company_name:
                logger.error(f"Could not find company with ID {company_id}")
                raise ValueError(f"Invalid company ID: {company_id}")

            safe_name = safe_filename(company_name)
            reviews_file = os.path.join("reviews", f"reviews_{safe_name}.json")
            summary_file = os.path.join("summaries", f"summary_{safe_name}.txt")

            if os.path.exists(summary_file):
                overwrite = (
                    input(
                        f"Summary file '{summary_file}' already exists. "
                        "Regenerate? [y/N]: "
                    )
                    .strip()
                    .lower()
                )
                if overwrite != "y":
                    logger.info("Skipping summary regeneration as per user choice.")
                    sys.exit(0)

            reviews = scrape_reviews(session, company_id, logger, max_pages=15)
            with open(reviews_file, "w", encoding="utf-8") as f:
                json.dump(reviews, f, ensure_ascii=False, indent=2)
            logger.info(f"Saved {len(reviews)} reviews to {reviews_file}")

            if reviews:
                summary = summarize_reviews(
                    client,
                    logger,
                    company_name,
                    company_intro,
                    [r["content"] for r in reviews],
                )
                with open(summary_file, "w", encoding="utf-8") as f:
                    f.write(summary)
                logger.info(f"Saved summary to {summary_file}")
                print(f"\n{summary}")
            else:
                logger.warning("No reviews found for summarization")

    except KeyboardInterrupt:
        logger.info("Process interrupted by user")
        sys.exit(1)
    except Exception:
        logger.exception("An error occurred")
        sys.exit(1)
